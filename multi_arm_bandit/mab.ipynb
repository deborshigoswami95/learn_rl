{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331de75f-4be9-4d31-a681-84fb1773f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e915e-d175-472f-baf6-ef9dbcde0d0c",
   "metadata": {},
   "source": [
    "# Problem Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de6c243-7d80-4df8-835b-a65aac2c9e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Pulled Arm 2, Reward = 0\n",
      "Round 2: Pulled Arm 2, Reward = 0\n",
      "Round 3: Pulled Arm 0, Reward = 0\n",
      "Round 4: Pulled Arm 1, Reward = 1\n",
      "Round 5: Pulled Arm 2, Reward = 1\n",
      "Round 6: Pulled Arm 0, Reward = 0\n",
      "Round 7: Pulled Arm 2, Reward = 1\n",
      "Round 8: Pulled Arm 1, Reward = 0\n",
      "Round 9: Pulled Arm 1, Reward = 1\n",
      "Round 10: Pulled Arm 0, Reward = 0\n",
      "\n",
      "Total Reward: 4\n",
      "Average Reward: 0.4\n",
      "Regret: 3.0\n"
     ]
    }
   ],
   "source": [
    "# A single arm of Multi-Arm Bandit problem where each arm gives rewards according to a bernoulli probability distribution\n",
    "class BernoulliBandit:\n",
    "    \"\"\"\n",
    "    Represents a single Bernoulli-distributed arm.\n",
    "    Reward = 1 with probability p, else 0.\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        self.p = p  # true success probability\n",
    "\n",
    "    def pull(self):\n",
    "        \"\"\"Simulate pulling the arm.\"\"\"\n",
    "        return np.random.rand() < self.p\n",
    "\n",
    "\n",
    "class BanditEnvironment:\n",
    "    \"\"\"\n",
    "    Multi-armed bandit environment.\n",
    "    Handles K arms, each with unknown probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, probabilities):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            probabilities (list or np.array): true success probabilities of each arm\n",
    "        \"\"\"\n",
    "        self.arms = [BernoulliBandit(p) for p in probabilities]\n",
    "        self.num_arms = len(probabilities)\n",
    "        self.best_arm = np.argmax(probabilities)\n",
    "        self.best_p = np.max(probabilities)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset counters and logs.\"\"\"\n",
    "        self.total_reward = 0\n",
    "        self.timestep = 0\n",
    "        self.rewards_history = []\n",
    "        self.actions_history = []\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Pull an arm and return reward.\"\"\"\n",
    "        reward = self.arms[action].pull()\n",
    "        self.total_reward += reward\n",
    "        self.timestep += 1\n",
    "        self.rewards_history.append(reward)\n",
    "        self.actions_history.append(action)\n",
    "        return reward\n",
    "\n",
    "    def average_reward(self):\n",
    "        return np.mean(self.rewards_history) if self.rewards_history else 0.0\n",
    "\n",
    "    def regret(self):\n",
    "        \"\"\"\n",
    "        Compute cumulative regret up to current timestep.\n",
    "        \"\"\"\n",
    "        optimal_total = self.best_p * self.timestep\n",
    "        actual_total = np.sum(self.rewards_history)\n",
    "        return optimal_total - actual_total\n",
    "\n",
    "\n",
    "true_probs = [0.2, 0.5, 0.7]\n",
    "\n",
    "env = BanditEnvironment(true_probs)\n",
    "\n",
    "# Example: manually pull arms\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "for t in range(10):\n",
    "    action = np.random.choice(env.num_arms)  # random choice for now\n",
    "    reward = env.step(action)\n",
    "    print(f\"Round {t+1}: Pulled Arm {action}, Reward = {int(reward)}\")\n",
    "\n",
    "print(\"\\nTotal Reward:\", env.total_reward)\n",
    "print(\"Average Reward:\", env.average_reward())\n",
    "print(\"Regret:\", env.regret())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abe824-145b-4c8a-bc7e-1ff67bc51e45",
   "metadata": {},
   "source": [
    "# Bandit Algorithms Explainer\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You are in a casino and playing slots at a machine. The machine has a number of arms, each of which gives rewards with a certain probability. You don't know what the probability distribution truly looks like but you know there is definitely a winning arm that will maximize your rewards. You have to come up with an algorithm that pulls arms and overtime converges on the arm that maximizes rewards. How do you write such an algorithm?\n",
    "\n",
    "While the basic problem framing is written above, it can get arbitrarily complex very quickly. For example what if the reward probability are not static over time? The game of writing a good RL algorithm lies in thinking not just of the basic algorithm but also how that algorithm would generalize over increasingly difficult problems while mathematically & empirically guaranteeing the fastest solution.\n",
    "\n",
    "## Epsilon Greedy Algorithm\n",
    "\n",
    "The formulation is the most basi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3abc23b4-b696-4092-bc21-fc86d574cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyBandit():\n",
    "    def __init__(self, env, epsilon = None):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.step = 1\n",
    "        self.mean_reward = np.zeros(shape = self.env.num_arms, dtype = np.float32)\n",
    "        self.arm_pull_count = np.zeros(shape = self.env.num_arms, dtype = np.float32)\n",
    "        self.net_reward = 0.0\n",
    "        if not self.epsilon:\n",
    "            self.ep_0 = 1.0\n",
    "        else:\n",
    "            self.ep_0 = epsilon\n",
    "        self.env.reset()\n",
    "        \n",
    "\n",
    "    def get_epsilon_with_decay(self):\n",
    "        return self.ep_0 / self.step\n",
    "\n",
    "    \n",
    "    def update_mean_reward_estimate(self, arm_idx, reward):\n",
    "        self.mean_reward[arm_idx] = self.mean_reward[arm_idx] + (1/(self.arm_pull_count[arm_idx] + 1)) * (reward - self.mean_reward[arm_idx])\n",
    "\n",
    "    \n",
    "    def action(self):\n",
    "        epsilon_at_step = self.get_epsilon_with_decay()\n",
    "        decision = np.random.rand() < epsilon_at_step # gets 1 with probability epsilon_at_step and 0 with probability ( 1 - epsilon_at_step )\n",
    "        if decision:\n",
    "            action = np.random.choice(env.num_arms)\n",
    "        else:\n",
    "            action = np.argmax(self.mean_reward)\n",
    "        reward = env.step(action)\n",
    "        self.update_mean_reward_estimate(action, reward)\n",
    "        self.step+=1\n",
    "        self.arm_pull_count[action]+=1\n",
    "\n",
    "\n",
    "    def run_bandit(self, total_timesteps = 1000):\n",
    "        self.reset()\n",
    "        for t in range(total_timesteps):\n",
    "            self.action()\n",
    "        print(f'Arm with max rewards: {np.argmax(self.mean_reward)}')\n",
    "\n",
    "\n",
    "class UCBBandit():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d664f2e1-617d-42e7-a167-c484017eab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm with max rewards: 0\n"
     ]
    }
   ],
   "source": [
    "true_probs = [0.9, 0.5, 0.1]\n",
    "\n",
    "env = BanditEnvironment(true_probs)\n",
    "bandit = EpsilonGreedyBandit(env)\n",
    "\n",
    "bandit.run_bandit(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a7968-7070-4f78-ab06-649141533d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python:(rl)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
